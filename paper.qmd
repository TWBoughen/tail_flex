---
title: Tail Flexibility in the Degrees of Preferential
 Attachment Networks
format:
  nature-pdf:
    fig-pos: 'H'
    # journal.cite-style is included in the tex file but ignored by pandoc if 
    # cite-method is not `natbib`.
    journal:
      cite-style: sn-basic
    # `citeproc` is the pandoc default. Set `cite-method: natbib` if required
    # to use the bst styles from the upstream template.
    cite-method: natbib
    keep-tex: true
    equal-margins: false
    number-sections: true
author:
  - name: Thomas William Boughen
    affiliations:
      - name: Newcastle University, UK
        id: 1
        department: School of Mathematics, Statistics and Physics
  - name: Clement Lee
    affiliations:
      - ref: 1
  - name: Vianey Palacios Ramirez
    affiliations:
      - ref: 1
abstract: |
  Devising the underlying generating mechanism of a real-life network is difficult as, more often than not, only its snapshots are available, but not its full evolution. One candidate for the generating mechanism is preferential attachment which, in its simplest form, results in a degree distribution that follows the power law. Consequently, the growth of real-life networks that roughly display such power-law behaviour is commonly modelled by preferential attachment. However, the validity of the power law has been challenged by the presence of alternatives with comparable performance, as well as the recent findings that the right tail of the degree distribution is often lighter than implied by the body, whilst still being heavy. In this paper, we study a modified version of the model with a flexible preference function that allows super/sub-linear behaviour whilst also guaranteeing that the limiting degree distribution has a heavy tail. We relate the distributions tail heaviness directly to the model parameters, allowing direct inference of the parameters from the degree distribution alone.
keywords: [networks, discrete extremes, degree distribution]
bibliography: refs.bib  

---
```{r,echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(paletteer)
library(gridExtra)
library(latex2exp)
library(ggpubr)
library(Rcpp)
library(RcppArmadillo)
```

# CL: Overall comments {-}

1. The introduction section is not quite clear and needs a lot of improvement, so here are my suggestions on the outline, in a hopefully more logical flow:
   a. Start with the broad picture e.g. that networks exist in many fields (name some for which you can cite applications), and that statistical methods are have been used to study networks. Some are for revealing the latent / underlying structures of the networks (stochastic block models), some are for explaining some response variables by network properties (ERGMs), and some are for postulating how the network grows and evolves (mechanistic models). Find some references, either the groundbreaking ones or comprehensive reviews to back this up.
   b. Introduce the general PA model as one of the mechanistic models. This will be a natural point to mention the rules (in terms as simple as possible) and introduce the preference function. Mention the special case when $b(k)=k$ and cite @Barabasi99.
   c. Then talk about PA model and mechanistic models in general being useful to **simulate** networks but not be fitted to real data because usually only snapshots are available. This leads well into the point on the degree distribution being informative.
   d. Then in the paragraph starting "When looking at ...", point out that it is the BA model [@Barabasi99] that leads to a power law degree distribution.
   e. Then point out that while it's attempting to attribute to the BA model when degrees seemingly follow the power law, there are two main concerns: first, a general preference function doesn't lead to the power law [@krapivsky01]; second, some claim that a lot of real data don't actually follow the power law. Mention not only @Broido_2019 but also responses to them e.g. @vvvk19.
   f. @vvvk19 leads naturally to the point that extreme value methods have not been used in analysing degree distributions until recently, such as @vvvk19 and @lef24. Not using these methods essentially downweights the deviation of the vertices with large degrees, which have far more influence on the network than those with small degrees, from the power law behaviour.
   g. Also mention Resnick's works (and others) on the crossroads of networks and extremes. Summarise what these papers do.
   h. Come back to the focus on the degree distribution, and point out that most methods, whether extremes are incorporated or not, are descriptive in the sense that no information about the preference function $b(\cdot)$ is revealed, even if PA model was indeed the underlying mechanism.
   i. Then mention that this paper addresses that gap. Specifically, **given the degree distribution of the network assumed to come from the PA model, can we directly infer the model parameters?** While @rudas07 have derived the limiting degree distribution in terms of $b(\cdot)$ and its parameters, we build upon it by finding the class of preference functions that result in realistic tails (heavy but not as heavy as implied by the power law).
   j. Then go straight to the division of the rest of the paper, and conclude the introduction section. The remaining materials should mostly go to the next section, but you can retain some materials if you think they complement (or already address) the above points.
2. Rough outline of Section 2, which should be on the PA model and deriving the tail heaviness of the limiting degree distribution in terms of $b(\cdot)$:
   a. Start with the general description of the PA model. You can move here the paragraphs in the introduction section, starting with "The network generative model ...", "Starting at time ...", and "Special cases of this model ...". Most of what you have written up till the pmf can be kept.
   b. Next, first state that of interest is the tail heaviness (which here refers to the shape parameter of the (integer) GPD approximation of the tail). Then move here the paragraphs on @shimura12 from the introduction.
   c. Then state proposition 2.1 and point to the Appendix for the proof.
   d. Next, we need a bit more discussion and explanation before stating that $b(\cdot)$ has to be asymptotically linear for a heavy tail. First point out that this result covers and aligns with the previously obtained result i.e. @krapivsky01. This is pretty much the paragraph currently under Section 2.1, but with a better flow. You might even include the special case of the special case i.e. $b(k)=k+\epsilon$, to briefly illustrate how a tail heaviness of $0.5$ is obtained for the BA model.
   e. Now, argue that $b(\cdot)$ has to be (asymptotically) linear above a threshold, in order to have a positive tail heaviness, then introduce the proposed preference function as our model, with tail heaviness $\beta/\lambda^{*}$.
   f. Conclude the section by saying that we will examine its property in the next Section (see next point).
3. The section could be called "Preferential attachment with flexible heavy tail", essentially making the subsection a section itself.
   a. Most of the materials can be kept, and the form of the IGPD should be inserted when approximating the survival using Stirling's (not Sterling's) approximation. The survival of the IGPD was in the introduction but I suggest moving it here.
   b. The piecewise linear model doesn't add much, so I would consider excluding it unless there's a point e.g. computational simplicity?
   c. I would conclude the section by moving the likelihood expression here, and say that the parameters can be inferred in a standard way, via the frequentist or Bayesian approach, as the likelihood is an explicit function of the parameters.
4. For simplicity, it's easier to use $m=1$ throughout and only mention the general case $m>1$ as future work in the discussion section. Also, it might be worth just calling the general preferential attachment model without the word "general" throughout, as it is unambiguous that it is the model with a flexible / general $b(\cdot)$, while the BA model is the one with $\b(k)=k+\epsilon$.
5. There are inconsistencies and duplications in various places, probably because you edited at one place and didn't update associated instances elsewhere. Do go back and forth when making edits.
6. The presentation can use many incremental improvements:
   a. use $k$ or $n$ for the integer but not both, unless necessary and in a consistent way. There were places where $n$ was used as the subject initially but it changed to $k$.
   b. ensure that the text in plots are legible.
   c. when citing, use the parentheses correctly, as they have been used inappropriately; in Quarto syntax, the square brackets will automatically put the parentheses (similar to citep in LaTeX).
   d. ensure that the maths don't over the linewidth.
   e. introduce the full term with the acronym in parentheses when using it for the first time, e.g. GPA.
   f. some references of equations are not working e.g. eq-surv?
   g. unless necessary, avoid strong adverbs / adjectives.
7. It seems that you version control all the files, which I won't recommend. Only track the files that you manually change, not the generated ones (cache, pdf, and tex unless you manually created one). This will facilitate the reproducibility. I have removed the generated files in my branch.
8. Consider creating subfolders to store the results, data, figures, etc. This will streamline your file structure. I have accordingly edited the R scripts so that if you run them, the rds files will be stored in results/.
9. Accents in names in bib / tex / qmd might not work on some computers. Here's the conversion chart: https://www.bibtex.org/SpecialSymbols/


# Introduction {#sec-intro}

The degree distribution of a network can be very informative about the networks structure, giving some understanding about the inequality in the influence of the nodes. Often the degree distribution is one of the simplest places to look when trying to understand this, as the full evolution of the network is usually not available. This makes modelling the degree distribution an important topic in research with contributions such as [ref]. 

When looking at the degree distributions of real networks, a power law is an attractive option to use when modelling as it is seemingly observed in a lot of networks. Fitting the discrete power law to the degree distribution come with the added benefit of suggesting preferential attachment as the generating mechanism since it has been shown to generate networks with a power law degree distribution.

Whether the power law is adequate for real degree distributions has become a heated debate, with alternatives such as @Broido_2019 seeming to outperform the power law in some cases. Additionally, @lef24 find that for many real degree distributions the body may follow a power law, the right tail does not or at least seems to be lighter than is implied by the body whilst still being heavy. One method for modelling the tail of the degree distributions of real networks is to use a discretised variation of the generalised Pareto distribution (GPD) usually dubbed the Integer GPD (GPD) where $X|X>v \sim \mathrm{IGPD}(\xi,\sigma,v)$ is defined by the survival:


$$
\Pr(X> x|X> v) = \left(\frac{\xi(x-v)}{\sigma} + 1\right)^{-1/\xi},\qquad x=v+1,v+2,\ldots
$$
for $v\in\mathbb Z^+, \sigma>0,\xi\in \mathbb R$.

[CL: Is the precise form of the IGPD needed here? The big picture is probably more important (point 3f in overall comments)]

@shimura12 introduces a quantity that will help in determining what domain of attraction a discrete distribution belongs to, that is:

For a distribution $F$ with survival function $\bar F$ and some $n\in\mathbb Z^+$ let:

$$
\Omega(F,n) = \left(\log\displaystyle\frac{\bar F (n+1)}{\bar F (n+2)}\right)^{-1} - \left(\log\displaystyle\frac{\bar F (n)}{\bar F (n+1)}\right)^{-1}
$$

@shimura12 then states that if $\lim_{n\rightarrow\infty} \Omega(F,n) = 1/\alpha$ ($\alpha>0$), then $F$ is heavy tailed with $\bar F(n) \sim n^{-\alpha}$. Additionally, if $\lim_{n\rightarrow\infty} \Omega(F,n) = 0$ then the distribution is light tailed.

[CL: The results of @shimura12 could be moved closer to Proposition 2.1. In the introduction, if you want to include @shimura12, you can mention the paper in passing, when outlining your approach, e.g. "Using the results by @shimura12, we derive the tail index / heaviness of the limiting degree distribution of the GPA model [@rudas07]. This informs us the precise class of preference functions that result in flexible heavy tail. "]

These models that seem to outperform the power law do not however come with the added benefit of suggesting a generating mechanism for the networks in the same way the power law does. Efforts to make the preferential attachment model more flexible have not yet remedied this issue with @krapivsky01 finding that using a sub/super-linear preference function yields a Weibull tail and a degenerate network, where one vertex eventually gains all new edges, respectively.

The network generative model that we will be focussing on in this paper is dubbed General Preferential Attachment in @rudas07 and is defined as follows:

Starting at time $t=0$ with an initial network of $m$ vertices that each have no edges, at times $t=1,2,\ldots$ a new vertex is added to the network bringing with it $m$ directed edges (with the new vertex as the source); the target for each of these edges are selected from the vertices already in the network with weights proportional to some function $b$ of their degree.

Special cases of this model include the Barabási-Albert (BA) model when $b(k) = k+\varepsilon$, which leads to a power-law degree distribution with index 2 and the Uniform Attachment (UA) model where $b(k)=c$ leading to a degree distribution in the Gumbel maximum domain of attraction.


Estimating the preference function used by network generative models like this is of great interest, with the potential to provide a deeper understanding of the relationships between the vertices in the network. Several papers and an R package (`PAfit`) have been dedicated to estimating the preference function that may have been used when a network was growing; one thing all these have in common is requiring access to a networks evolution data, something that is not always readily available or feasible to access. A method for estimating a preference function without this data available would be extremely useful.

@rudas07 introduces theoretical results for this model with a general preference function in the cases that the network being generated is a tree ($m=1$); presenting an opportunity to design a preference function that will capture the behaviours observed in the degree distributions of real networks. Since they provide theoretical results for the limiting degree distribution, a model of this form would be able to be fitted to the degree distributions of real networks where the full evolution is not available, only a snapshot.

The rest of this paper is as follows: @sec-gpa looks at the theoretical limiting behaviour of a GPA model when $m=1$ according to the results from @rudas07, going on to introduce a flexible class of preference functions that can guarantee a heavy tailed degree distribution while still being flexible in the body. @sec-rec shows that the preference function parameters can be fairly well recovered from the degree distributions of networks simulated from the GPA model using various functions of the class introduced in @sec-gpa. Having shown that for simulated data the model parameters are recoverable from the degree distribution alone, @sec-real builds upon this and attempts to fit the model to degree distributions of real networks; modelling not only the degree distribution itself but also the estimates for the parameters of the preference function assuming the networks evolved according to the GPA scheme. @sec-conc discusses the main results, proposes future work and concludes the paper.


# General Preferential Attachment Model {#sec-gpa}

In this section, we introduce a general preferential attachment model, with the focus on the limiting degree distribution as a function of the preference function $b(\cdot)$ where each vertex brings with it $m$ edges. This preference function is subject to the following conditions:

$$
b:\mathbb N \mapsto \mathbb R^+\setminus\{0\},
$$ 

$$
\sum_{k=0}^\infty\frac{1}{b(k)} = \infty.
$$ {#eq-condb2}

Given these two conditions an expression for the survival of the limiting degree distribution can be found in the case that $m=1$; obtained by considering a branching process that is equivalent to the growth of the network, as in @rudas07. Theorem 1 from @rudas07 states that for the tree $\Upsilon(t)$ at time $t$:

$$
\lim_{t\rightarrow\infty}\frac{1}{|\Upsilon(t)|}\sum_{x\in\Upsilon(t)}\varphi(\Upsilon(t)_{\downarrow x}) = \lambda^* \int_0^\infty e^{-\lambda^* t}\mathbb E\left[\varphi(\Upsilon(t))\right]dt
$$
where $\lambda^*$ satisfies $\hat\rho(\lambda^*)=1$. (CL: $\hat{\rho}$ has not been defined yet.)

The limiting survival can be viewed as the limit of the empirical proportion of vertices with degree over a threshold $k$, that is:

$$
\bar F(k) = \lim_{t\rightarrow\infty}\frac{\sum_{x\in\Upsilon(t)}\mathbb I\left\{\text{deg}(x,\Upsilon(t)_{\downarrow x})>k\right\}}{\sum_{x\in\Upsilon(t)} 1}
$$
which by the previously stated theorem can also be written as:

$$
\bar F(k) = \frac{\int_0^\infty e^{-\lambda^* t}\mathbb E\left[\mathbb I\left\{\text{deg}(x,\Upsilon(t))>k\right\}\right]dt}{\int_0^\infty e^{-\lambda^* t}dt} = \prod_{i=0}^k\frac{b(i)}{\lambda^* + b(i)}.
$$
Using the fact that $f(k) = \bar F(k-1) - \bar F(k)$ the probability mass function (pmf) is

$$
f(k) = \frac{\lambda^*}{\lambda^* + b(k)}\prod_{i=0}^{k-1}\frac{b(i)}{\lambda^*+b(i)},\qquad k\in\mathbb N,
$$

which aligns with the result from @rudas07, where the expression is derived using the same branching process results. 

Using results from @shimura12, the domain of attraction of this limiting degree distribution can be found.

:::{#prp-omega}
If $b(k) \rightarrow \infty$ as $k\rightarrow \infty$ then
$$
\bar F(k) = \prod_{i=0}^k\frac{b(i)}{\lambda^* + b(i)}
$$
and 

$$
\lim_{k\rightarrow\infty}\Omega(F,k) = \lim_{k\rightarrow\infty}\frac{b(k+1)-b(k)}{\lambda^*}.
$$
:::

@prp-omega implies that the only way to obtain a heavy tailed degree distribution using a non-decreasing preference function is to have that preference function be asymptotically linear. The next subsection focuses on a particular class of these preference functions that provide flexible behaviour but guarantee a  heavy tail.

## Preferential Attachment with flexible heavy tail {#sec-model}

In @krapivsky01, a preference function of the form $k^\alpha +\varepsilon$ has been considered. However, it has been noted that super-linear preferential attachment ($\alpha > 1$) leads to a degenerate limiting degree distribution where at some point one vertex gains the connection (CL: edge?) from every vertex that joins the network, additionally sub-linear preferential attachment ($\alpha <1$) has been shown to lead to a light tailed limiting degree distribution. Consider a preference function of the form:

$$
b(k) = \begin{cases}
k^\alpha + \varepsilon,&k<k_0,\\
k_0^\alpha + \varepsilon + \beta(k-k_0), &k\ge k_0,
\end{cases}
$$ for $\alpha,\beta, \varepsilon>0$ and $k_0\in\mathbb N$.

Using a preference function with guaranteed linear behaviour in the limit, allows for the inclusion of sub/super linear behaviour without losing the heavy tails or ending up with a degenerate degree distribution.

The limiting degree distribution resulting from using a preference function of this form can be found to have survival:

$$
\bar F(k) = \begin{cases}
\displaystyle\prod_{i=0}^{k}\frac{i^\alpha + \varepsilon}{\lambda^*+i^\alpha + \varepsilon},&k<k_0,\\
\displaystyle\left(\prod_{i=0}^{k_0-1}\frac{i^\alpha + \varepsilon}{\lambda^*+i^\alpha + \varepsilon}\right)\frac{\Gamma(\lambda^*+k_0^\alpha + \varepsilon)/\beta)}{\Gamma\left((k_0^\alpha + \varepsilon)/\beta\right)} \frac{\Gamma\left(k-k_0 + 1 +\frac{k_0^\alpha + \varepsilon}{\beta}\right)}{\Gamma\left(k-k_0 + 1 +\frac{\lambda^* +k_0^\alpha + \varepsilon}{\beta}\right)},&k\ge k_0,
\end{cases}
$${#eq-polysurv}

with $\lambda^*$ satisfying $\hat{\rho}(\lambda^{*})=1$, where

$$
\hat\rho(\lambda) = \sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{i^\alpha + \varepsilon}{\lambda+i^\alpha + \varepsilon} + \left(\frac{k_0^\alpha + \varepsilon}{\lambda-\beta}\right)\prod_{i=0}^{k_0-1}\frac{i^\alpha + \varepsilon}{\lambda + i^\alpha + \varepsilon} = 1,
$$

which has to be solved numerically for most parameter choices. Note that $\lambda$ has to be greater than $\beta$ since when $\lambda\le \beta$ the infinite sum diverges. (CL: Where is the infinite sum?)

Some examples of what the degree distribution looks like are shown below in @fig-polylinsurv:

```{r, echo=FALSE}
source('funcs.R')
```

```{r, echo=FALSE, warning=FALSE, cache=TRUE, out.width="95%"}
#| fig-width: 8
#| fig-height: 4
#| fig-cap: Theoretical survival distributions of the limiting degree distributions, according to various combinations of $(\alpha, \beta, \varepsilon)$ and $k_0=20$ of the proposed preferential attachment model.
#| label: fig-polylinsurv

as = c(0.5,1,1.5)
bs = c(0.1,0.5,1,1.5)
eps = c(.01,.1,.5,1)
k0s = 20
x = seq(0, 2000, by = 10)
pars = expand.grid(as,bs,eps,k0s,x)
names(pars) = c('a','b','eps','k0','x')
lambdas = numeric(nrow(pars))
for(i in 1:nrow(pars)){
  lambdas[i] = find_lambda2(polylin(pars$a[i], pars$eps[i]), pars$b[i], pars$k0[i])
}
pars = cbind(pars,lambdas)
names(pars) = c('a', 'b', 'eps', 'k0','x','lambda')
surv =numeric(nrow(pars))
for(i in 1:nrow(pars)){
  surv[i] = S(pars$x[i], polylin(pars$a[i], pars$eps[i]),pars$lambda[i], pars$k0[i],pars$b[i])
}
pars = cbind(pars,surv)
names(pars) = c('a', 'b', 'eps', 'k0','x','lambda','surv')

library(latex2exp)
labeller = label_bquote(rows = `epsilon`==.(eps),cols = `alpha`==.(a))

ggplot(data = pars) + geom_line(aes(x=(x+1),y=surv, linetype=as.character(b),colour =as.character(b))) +
  scale_x_log10() + scale_y_log10(limits=c(1e-8,1))+theme(aspect.ratio = 1/2) + theme_bw() + xlab('Total Degree')  +ylab('Survival') + labs(linetype=TeX('\\beta'), colour =TeX('\\beta')) + 
  facet_grid(eps~a,labeller = labeller, scales='free')
```

Applying Stirling's approximation of the gamma function to the second case of @eq-polysurv, we obtain, for $k\geq k_0$,

<!-- $$ -->
<!-- \frac{\Gamma(x+y)}{\Gamma(x)}\approx x^{y}, -->
<!-- $$ -->
<!-- and utilising the expression from @eq-polysurv: -->

<!-- \begin{align*} -->
<!-- \bar F(k|k\ge k_0) &= \frac{\Gamma\left(\frac{\lambda^* + k_0^\alpha + \varepsilon}{\beta}\right)}{\Gamma\left(\frac{k_0^\alpha + \varepsilon}{\beta}\right)}\times\frac{\Gamma\left(k-k_0  +1 + \frac{k_0^\alpha + \varepsilon}{\beta}\right)}{\Gamma\left(k-k_0  +1 + \frac{\lambda^*+ k_0^\alpha + \varepsilon}{\beta}\right)}\\ -->
<!-- &\approx\left(\frac{k_0^\alpha+\varepsilon}{\beta}\right)^{\lambda^*/\beta}\left(k-k_0+1+\frac{k_0^\alpha + \varepsilon}{\beta}\right)^{-\lambda^*/\beta}\\ -->
<!-- &=\left(\frac{k_0^\alpha+\varepsilon}{k_0^\alpha+\varepsilon + \beta}\right)^{\lambda^*/\beta}\left(\frac{\beta(k-k_0)}{\beta + k_0^\alpha+\varepsilon} + 1\right)^{-\lambda^*/\beta}\\ -->
<!-- &=\left(\frac{\beta(k+1-k_0)}{k_0^{\alpha}+\varepsilon} + 1\right)^{-\lambda^{*}/\beta} -->
<!-- \end{align*} -->


$$
\bar F(k) 
%\begin{cases}
%=\displaystyle\prod_{i=0}^{k}\frac{i^\alpha + \varepsilon}{\lambda^*+i^\alpha + \varepsilon},&k<k_0,\\
\displaystyle\approx \left(\prod_{i=0}^{k_0-1}\frac{i^\alpha + \varepsilon}{\lambda^*+i^\alpha + \varepsilon}\right) \left(\frac{\beta(k+1-k_0)}{k_0^{\alpha}+\varepsilon} + 1\right)^{-\lambda^*/\beta},%&k\ge k_0,
%\end{cases}
$$ which is the survival function of the $\text{IGPD}\displaystyle\left(\frac{\beta}{\lambda^*}, \frac{k_0^\alpha + \varepsilon}{\lambda^*},k_0-1\right)$ distribution. This means that, above the threshold of the piecewise preference function, the limiting degree distribution can be approximated by the IGPD with shape parameter $\beta/\lambda^{*}$.

To assess the quality of the approximation, the conditional survivals are shown in @fig-approx_surv in colour and their IGPD approximations are shown in grey. The approximation seems to hold up fairly well even for large degrees.


```{r, echo=FALSE, warning=FALSE, cache=TRUE, out.width="95%"}
#| fig-width: 8
#| fig-height: 4
#| fig-cap: Theoretical conditional survivals (grey) alongside their IGPD approximations (coloured).
#| label: fig-approx_surv

as = c(0.5,1,1.5)
bs = c(0.1,0.5,1,1.5)
eps = c(.01,.1,.5,1)
k0s = 20
x = k0s:200
pars = expand.grid(as,bs,eps,k0s,x)
names(pars) = c('a','b','eps','k0','x')
lambdas = numeric(nrow(pars))
for(i in 1:nrow(pars)){
  lambdas[i] = find_lambda2(polylin(pars$a[i], pars$eps[i]), pars$b[i], pars$k0[i])
}
pars = cbind(pars,lambdas)
names(pars) = c('a', 'b', 'eps', 'k0','x','lambda')
surv =numeric(nrow(pars))
gp_surv =numeric(nrow(pars))
for(i in 1:nrow(pars)){
  surv[i] = S(pars$x[i], polylin(pars$a[i], pars$eps[i]),pars$lambda[i], pars$k0[i],pars$b[i])/
    S(pars$k0[i], polylin(pars$a[i], pars$eps[i]),pars$lambda[i], pars$k0[i],pars$b[i])
  gp_surv[i] = mev::pgp(pars$x[i],pars$k0[i]-1,((pars$k0[i]+1)^pars$a[i] +pars$eps[i]-1 )/pars$lambda[i],pars$b[i]/pars$lambda[i],lower.tail = F)
}
pars = cbind(pars,surv,gp_surv)
names(pars) = c('a', 'b', 'eps', 'k0','x','lambda','surv','gp_surv')

library(latex2exp)
labeller = label_bquote(rows = `epsilon`==.(eps),cols = `alpha`==.(a))

library(ggh4x)

l1 = scale_y_log10(limits=c(1e-1,1))
l2 = scale_y_log10(limits=c(1e-2,1))
l3 = scale_y_log10(limits=c(1e-3,1))
l4 = scale_y_log10(limits=c(1e-4,1))

limlist = list(l1,l2,l3,l4)

ggplot(data = pars) + geom_line(aes(x=(x),y=surv, linetype=as.character(b),colour =as.character(b)), lwd=1) +
  geom_line(data = pars[pars$x>=pars$k0,],aes(x=(x+1),y=gp_surv, linetype=as.character(b)),colour ='grey')+
  scale_x_log10()+
  theme(aspect.ratio = 1/2) + theme_bw() + xlab('Total Degree')  +ylab('Survival') + labs(linetype=TeX('\\beta'), colour =TeX('\\beta')) +
  facet_grid(eps~a,labeller = labeller,scales='free') + facetted_pos_scales(y=limlist)

```



Since $\beta>0$, the shape parameter of the IGPD is positive and thus the distribution is heavy tailed. Additionally the value of the shape parameter $\xi$ is shown in @fig-polyheat for various parameter choices:


```{r, echo=FALSE, warning=FALSE, cache=TRUE, out.width="95%"}
#| fig-width: 12
#| fig-height: 6
#| fig-cap: Heatmaps of $\xi$ for various combinations of the parameters of the proposed model.
#| label: fig-polyheat
#| 
rho_optim_ba= Vectorize(function(a,eps, b, k0){
  return(abs(rho(2*b, polylin(a, eps), b, k0)-1))
},vectorize.args = 'a')
find_a_ba = Vectorize(function(b, eps, k0){
  out = optimise(rho_optim_ba, c(0.00001,3), b=b, eps=eps, k0=k0)$minimum
  return(out)
}, vectorize.args = 'b')
N =50
as = seq(0,2,l=N+1)[-1]
bs = seq(0,2,l=N+1)[-1]
eps = c(.01,.1,.5,1)
k0 = c(25,100)
pars = expand.grid(as,bs,eps,k0)
names(pars) = c('a', 'b', 'eps', 'k0')
lambdas = numeric(nrow(pars))
a_for_ba = numeric(nrow(pars))
for(i in 1:nrow(pars)){
  lambdas[i] = find_lambda2(polylin(pars$a[i], pars$eps[i]), pars$b[i], pars$k0[i])
  a_for_ba[i] = find_a_ba(pars$b[i], pars$eps[i], pars$k0[i])
}
pars = cbind(pars,lambdas,a_for_ba)
names(pars) = c('a', 'b', 'eps', 'k0','lambda','ba')
labeller = label_bquote(cols = `epsilon`==.(eps),rows = ~k[0]==.(k0))

ggplot(pars) + geom_raster(aes(x=a,y=b,fill=b/lambda)) +
  geom_line(aes(x=ba, y=b),linetype='dashed', colour='red',lwd=1)+
  scale_fill_paletteer_c(palette='grDevices::Blues',limits=c(0,1),direction=-1)+theme(aspect.ratio = 1)+ylim(min(bs), max(bs))+xlim(min(as), max(as))+
  facet_grid(k0~eps,labeller = labeller) + labs(fill=TeX('\\xi')) + xlab(TeX('\\alpha')) + ylab(TeX('\\beta')) + theme_minimal(15)


```

The darker regions on the heat maps correspond to a heavier tail and the lighter to a lighter tail, the red dashed line shows combinations of $\alpha$ and $\beta$ that produce a limiting degree distribution with the same tail heaviness as the Barabási-Albert model, $\xi=0.5$.

### Piecewise Linear Model

When $\alpha = 1$ the preference function simplifies to a piecewise linear function with $\beta$ representing the ratio of the gradients of the two components:

$$
b(k) = \min(k,k_0) + \varepsilon + \mathbb I\{k\ge k_0\}\beta(k-k_0)
$$ for some $\beta, \varepsilon, k_0>0$.

Following @eq-polysurv the limiting degree distribution is:

$$
\bar F(k) = \begin{cases}
\displaystyle\frac{\Gamma(\lambda^*+\varepsilon)\Gamma(k+1+\varepsilon)}{\Gamma(\varepsilon)\Gamma(k+1+\lambda^*+\epsilon)},&k<k_0,\\
\displaystyle\frac{\Gamma(\lambda^*+\varepsilon)\Gamma(k_0+\varepsilon)}{\Gamma(\varepsilon)\Gamma(k_0+\lambda^*+\varepsilon)}\times \frac{\Gamma\left(\frac{k_0+\varepsilon+\lambda^*}{\beta}\right)}{\Gamma\left(\frac{k_0+\varepsilon}{\beta}\right)}\times\frac{\Gamma\left(k-k_0 + \frac{k_0+\varepsilon}{\beta}+1\right)}{\Gamma\left(k-k_0 + \frac{k_0+\varepsilon+\lambda^*}{\beta}+1\right)},&k\ge k_0.
\end{cases}
$$ {#eq-linsurv}

Similar to the general case, the survival function can be approximated by:

$$
\bar F(k) \approx \begin{cases}
\displaystyle\frac{\Gamma(\lambda^*+\varepsilon)}{\Gamma(\varepsilon)} (1+\varepsilon)^{\lambda^*}\left(\frac{k}{1+\varepsilon} + 1\right)^{-\lambda^*},&k<k_0,\\
\displaystyle\frac{\Gamma(\lambda^*+\varepsilon)\Gamma(k_0+\varepsilon)}{\Gamma(\varepsilon)\Gamma(k_0+\lambda^*+\varepsilon)}\times\left(\frac{\beta(k+1-k_0)}{k_0+\varepsilon} + 1\right)^{-\lambda^*/\beta},&k\ge k_0.
\end{cases}
$$

Some examples of the limiting degree distributions are shown in the central column of @fig-polylinsurv.


# Simulation study {#sec-rec}

<!-- The goal of was to provide a possible preference function that is able to explain the growth of real networks with varied tail behaviour in their degree distributions. -->
This section aims to show that the parameters of the model in @sec-model can be recovered from the degree distribution of a network simulated from it.

The procedure for recovering the parameters begins with simulating a network from the model with $N=100,000$ vertices and $m=1$ given some set of parameters $\pmb\theta = (\alpha, \beta, \varepsilon, k_0)$, obtaining the degree counts in the form of a vector of degrees $\pmb x = (1,2,\ldots,M)$ and the number of vertices with those degrees $\pmb n$ where $M$ is the maximum degree. However, when it comes to fitting this model to real data in the next section, small degrees are extremely influential and will often cause the model to have a bad fit even though the rest of the data looks similar to that of the simulations; truncating the data above some value, say $l<k_0$, should allow the model to be more effectively fitted. This accounts in some way for the fact that this model only applies to trees, and could not possibly capture the behaviour for nodes with small degrees. Using the p.m.f derived in @rudas07, the likelihood given that $x_i \ge l$ for all $i =1,2,\ldots,M$ is:


\begin{align*}
L(\pmb x,\pmb n | \pmb \theta) = \left(\frac{\lambda^*}{\lambda^*+\varepsilon}\right)^{n_0}\left(\prod_{j=l}^{k_0-1}\frac{j^\alpha +\varepsilon}{\lambda^* + j^\alpha +\varepsilon}\right)^{\left(\sum_{x_i\ge k_0}n_{x_i}\right)} \prod_{l \le x_i<k_0}\left(\frac{\lambda^*}{\lambda^* +x_i^\alpha + \varepsilon } \prod_{j=l}^{k_0-1}\frac{j^\alpha + \varepsilon}{\lambda^* + j^\alpha + \varepsilon}\right)^{n_i}\\ \times \prod_{x_i\ge k_0}\left(\frac{\text{B}(x_i-k_0 + (k_0^\alpha + \varepsilon)/\beta,1+\lambda^*/\beta)}{\text{B}((k_0^\alpha + \varepsilon)/\beta,\lambda^*/\beta)}\right)^{n_i}
\end{align*}

where $B(y,z)$ is the the beta function.

This likelihood allows inference to be made about the parameters using a Bayesian approach using the priors:


\begin{align*}
\alpha&\sim \text{Ga}(1,0.01),\\
\beta &\sim  \text{Ga}(1,0.01),\\
k_0 &\sim \text{U}(1,10,000),\\
\varepsilon &\sim \text{Ga(1,0.01)},
\end{align*}

to obtain a posterior distribution that can then be used in an adaptive Metropolis-Hastings Markov chain Monte Carlo (MCMC) algorithm  to obtain posterior samples. The results of this inference are shown in @fig-rec1 and @fig-rec2. For these simulated networks $l=0$.

```{r, echo=FALSE, warning=FALSE, cache=FALSE, out.width="95%"}
#| fig-width: 8
#| fig-height: 8
#| fig-cap: Posterior estimates of survival function for data simulated from the proposed model with various combinations of ($\alpha,\beta,\varepsilon$) and $k_0=20$.
#| label: fig-rec1

surfit_list = list()
pars = readRDS('results/recovery_pars.rds')
recovery_list = readRDS('results/recovery_dat.rds')
selected = c(1,27,14,35)
# selected = 1:36
for(k in 1:length(selected)){
  j=selected[k]
  x = recovery_list[[j]]$mcmc$dat[,1]
  ls = c()

  y_975 = recovery_list[[j]]$mcmc$surv$CI[2,][order(x)]
  y_025 = recovery_list[[j]]$mcmc$surv$CI[1,][order(x)]
  y_50 = recovery_list[[j]]$mcmc$surv$est[order(x)]
  x = sort(x)
  surfit_list[[k]] = ggplot() + geom_point(data=twbfn::deg_surv(recovery_list[[j]]$degs), aes(x=degree, y=surv)) +
    geom_line(data=NULL, aes(x=!!x, y=!!y_975), colour = 'red', linetype='dashed')+
    geom_line(data=NULL, aes(x=!!x, y=!!y_50),colour='red')+
    geom_line(data=NULL, aes(x=!!x, y=!!y_025), colour = 'red', linetype='dashed')+
    scale_x_log10(limits=c(1,1e5))  +scale_y_log10(limits = c(1/length(recovery_list[[j]]$degs),1))+theme_bw() + theme(aspect.ratio = 0.66,axis.title.y=element_blank(),                                                                   axis.text.y=element_blank(),                                                                     axis.ticks.y=element_blank()) + xlab('')+ylab('') +
    ggtitle(TeX(paste0('$\\alpha = $',pars$a[j],
                       ', $\\epsilon = $',pars$eps[j],
                       ', $\\beta = $',pars$b[j])))
}

fig = ggpubr::ggarrange(plotlist = surfit_list,
          nrow=round(sqrt(length(surfit_list)),0),ncol=round(sqrt(length(surfit_list)),0),
          label.x='Degree', label.y = 'Survival')


ggpubr::annotate_figure(fig, bottom='Degree', left='Survival')


```



```{r fig-rec2, out.width="80%"}
#| echo: false
#| fig.cap: Posterior estimates of parameters for data simulated from the proposed model with various combinations of ($\alpha$,$\beta$,$\varepsilon$) and $k_0=20$.
knitr::include_graphics("mcmc_plot.jpg")
```


@fig-rec1 and @fig-rec2 demonstrate that using this methodology it is possible to recover the model parameters fairly well from only the final degree distribution of a simulated network. This indicates that the method may also be able to be applied to the degree distributions of real networks, estimating the model parameters assuming they evolved according to the GPA scheme.


# Application to Real Data {#sec-real}

Turning now to real data, the goal is to fit the model to the degree distributions of real networks from various sources. [describe networks being used]. Alongside fitting this model to the degree distributions, we then compare the fit to that of an mixture distribution that was used in @lef24 and is similar to others that have been used to model degree distributions [others]. 



```{r, echo=FALSE, warning=FALSE, cache=TRUE, out.width="80%"}
#| fig-width: 9
#| fig-height: 9
#| fig-cap: Posterior estimates (solid red) of survival for several real data sets and their 95% credible intervals (dotted red)
#| label: fig-real1

plt1 = readRDS('results/comp_plot.rds')

plot(plt1)

```

@fig-real1 displays the posterior estimates of the survival function for various data sets, obtained from fitting the GPA model and the Zipf-IGP mixture model. In most cases, the GPA model does not necessarily provide an improvement in fit when compared to the Zipf-IGP model but where the GPA model fits well we gain additional information about the preference function assuming that the network evolved according the the GPA scheme. @fig-shapes shows the posterior of the shape parameter $\xi$ obtained from the Zipf-IGP model alongside the posterior of the equivalent shape parameter $\beta/\lambda^*$ obtained from fitting the GPA model. Generally, the GPA model performs similarly to the Zipf-IGP when estimating the tail behaviour of the degree distribution and where it doesn't it appears to either be because it is fitting better at the tail than the Zipf-IGP model or because of the threshold being estimated as too low forcing almost all of the data to be modelled by the linear part of the GPA. This again shows the effects that small degrees have on this model, which is somewhat expected as the theory used for this model is for trees and none of these real networks (nor many real networks) are.  

```{r, echo=FALSE, warning=FALSE, cache=FALSE, out.width="80%"}
#| fig-width: 9
#| fig-height: 9
#| fig-cap: Posterior estimates (solid red) of survival for several real data sets and their 95% credible intervals (dotted red)
#| label: fig-shapes

plt2 = readRDS('results/shape_plot.rds')

plot(plt2)

```


@fig-pa shows the estimated preference function $b$ alongside the 95% credible interval  on a log-log plot. Although the credible interval becomes very large for the largest degrees, this is expected as not all of these networks had data in that region, for those that do the credible interval is much narrower as is the case for `sx-mathoverflow`. The most insightful conclusion we can draw from these plots come from the shape of the preference function. There appears to be two distinct shapes of preference function. The first appears mostly flat (similar to UA) for the smallest degrees and then after a threshold PA kicks in, some with this shape are `pajek-erdos` and `sx-mathoverflow`. The second distinct shape appears provide some clear PA behaviour that then slows down after a certain point, examples of this are seen in the two infrastructure networks `opsahl-openflights` and `topology`. 


```{r, echo=FALSE, warning=FALSE, cache=FALSE, out.width="80%"}
#| fig-width: 9
#| fig-height: 9
#| fig-cap: Posterior estimate for preference function (solid) with 95% credible interval (dashed) on log-log scale.
#| label: fig-pa

plt3 = readRDS('results/PA_plot.rds')
plot(plt3)

```


```{r fig-rec3, out.width="80%"}
#| fig-cap: Posterior estimates of parameters for real data.
#| echo: false
knitr::include_graphics("pars_plot.png")
```

# Conclusion and Discussion {#sec-conc}

In this paper we introduced a flexible class of preference function that when used under the GPA scheme (in the tree setting) is guaranteed to generate a network with a heavy tailed degree distribution whilst remaining flexible in the body. Using simulations from networks using this class of preference function we showed that the model parameters are quite easily estimated from the degrees alone at a snapshot in time. Seeing this we applied this method to the degree distributions of real networks, estimating their model parameters assuming they evolved in the same way. Not only did this yield fairly good fits for the degree distribution, similar to that of the Zipf-IGP, it came with the added benefit of giving a posterior estimate for a preference function. 

Obviously this method had its flaws in that the lowest degrees needed to be truncated as they had a very large effect on the fit of the model as a result of using theory developed for trees and applying it to general networks. Hopefully, as the field progresses we will be able to apply theory developed for general networks using a similar method to this, allowing us to compare the results here something that is more accurate.

GPA is also a fairly simplistic one on its own despite having large flexibility in the preference function allowing for edges to only be added at times when a new node is introduced into the network and not allowing for anything like vertex/edge death, reciprocal edges and rewiring steps. [@deijfen2015] introduces some theory for GPA with vertex death based on the work by [@rudas07], but is unable to come to an expression for the degree distribution. However, adding something like fitness scores into the GPA model seems fairly simple to do even going so far as making each of the parameters used in the class of functions here node-wise instead of universal. For example having the preference weight for a vertex be given by $b(k_i, \underline{\gamma_i})$  where $\underline{\gamma_i}\sim G$ is the vertex's fitness and $k_i$ is the vertex's degree, it is reasonable to expect that the degree distribution for a model like this to be given by:

$$
\bar F(k) = \mathbb E_G\left[\prod_{i=0}^k \frac{b(i, \gamma)}{\lambda^* + b(i,\gamma)}\right],\qquad \gamma\sim G,\, k=0,1,2,\ldots
$$

# Appendix {.appendix}

## Additional Results 
### Tail heaviness of general preferential attachment model


Recall the limiting survival function:

$$
\bar F(k) = \prod_{i=0}^k \frac{b(i)}{\lambda^*+b(i)},
$$

The aim is to determine how this distribution behaves for different choices of $b$, specifically what maximum domain of attraction does this distribution belong to and is it affected by the choice of preference function $b$. @shimura12 introduces a quantity that will help in determining what domain of attraction a discrete distribution belongs to, that is:

For a distribution $F$ with survival function $\bar F$ and some $n\in\mathbb Z^+$ let:

$$
\Omega(F,n) = \left(\log\displaystyle\frac{\bar F (n+1)}{\bar F (n+2)}\right)^{-1} - \left(\log\displaystyle\frac{\bar F (n)}{\bar F (n+1)}\right)^{-1}
$$

[@shimura12] then states that if $\lim_{n\rightarrow\infty} \Omega(F,n) = 1/\alpha$ ($\alpha>0$), then $F$ is heavy tailed with $\bar F(n) \sim n^{-\alpha}$. Additionally, if $\lim_{n\rightarrow\infty} \Omega(F,n) = 0$ then the distribution is light tailed.

Consider a preference function $b(\cdot)$ that fulfills:

$$
\lim_{n\rightarrow \infty} b(n) = \infty
$${#eq-condb1}

Substituting in the form of $\bar F(n)$ from @eq-surv and taking the limit, subject to @eq-condb1:

$$
\lim_{n\rightarrow\infty}\Omega(F,n) = \lim_{n\rightarrow\infty}\frac{b(n+2)-b(n+1)}{\lambda^*}.
$$ {#eq-omegares} (see appendix for full proof).

Whilst there are many classes of functions that satisfy @eq-condb1 and @eq-condb2, the vast majority of these result in $\Omega(F,n) \rightarrow 0$, meaning that the limiting degree distribution is light tailed. In fact, in order for the limiting degree distribution to be heavy tailed, $b$ must be asymptotically linear ($b(k)\sim k$).


## Proofs  

### Tail heaviness of GPA

Taking the form of the GPA degree survival: $$
\bar F(n) = \prod_{i=0}^n\frac{b(i)}{\lambda+b(i)}
$$ and substituting into the formula for $\Omega(F,n)$:

```{=tex}
\begin{align*}
\Omega(F,n)&=\left(\log\frac{\prod_{i=0}^{n+1}\frac{b(i)}{\lambda+b(i)}}{\prod_{i=0}^{n+2}\frac{b(i)}{\lambda+b(i)}}\right)^{-1}-\left(\log\frac{\prod_{i=0}^{n}\frac{b(i)}{\lambda+b(i)}}{\prod_{i=0}^{n+1}\frac{b(i)}{\lambda+b(i)}}\right)^{-1}\\
&=\left(\log\frac{\lambda+b(n+2)}{b(n+2)}\right)^{-1}-\left(\log\frac{\lambda+b(n+1)}{b(n+1)}\right)^{-1}\\
&=\left(\log\left[1+\frac{\lambda}{b(n+2)}\right]\right)^{-1}-\left(\log\left[1+\frac{\lambda}{b(n+1)}\right]\right)^{-1}
\end{align*}
```
Clearly if $b(n)=c$ or $\lim_{n\rightarrow\infty}b(n)=c$ for some $c>0$ then $\Omega(F,n)=0$. Now consider a non-constant $b(n)$ and re-write $\Omega(F,n)$ as:

```{=tex}
\begin{align*}
\Omega(F,n) &= \left(\log\left[1+\frac{\lambda}{b(n+2)}\right]\right)^{-1}-\frac{b(n+2)}{\lambda}+\frac{b(n+2)}{\lambda}-\left(\log\left[1+\frac{\lambda}{b(n+1)}\right]\right)^{-1}+\frac{b(n+1)}{\lambda}  -\frac{b(n+1)}{\lambda}\\
&=\left\{ \left(\log\left[1+\frac{\lambda}{b(n+2)}\right]\right)^{-1}-\frac{b(n+2)}{\lambda}\right\} - \left\{ \left(\log\left[1+\frac{\lambda}{b(n+1)}\right]\right)^{-1}-\frac{b(n+1)}{\lambda}\right\}+\frac{b(n+2)}{\lambda}-\frac{b(n+1)}{\lambda}
\end{align*}
```
Then if $\lim_{n\rightarrow\infty}b(n)=\infty$ it follows that:

```{=tex}
\begin{align*}
\lim_{n\rightarrow\infty}\Omega(F,n) &= \lim_{n\rightarrow\infty}\left\{ \left(\log\left[1+\frac{\lambda}{b(n+2)}\right]\right)^{-1}-\frac{b(n+2)}{\lambda}\right\} - \lim_{n\rightarrow\infty}\left\{ \left(\log\left[1+\frac{\lambda}{b(n+1)}\right]\right)^{-1}-\frac{b(n+1)}{\lambda}\right\}\\
&\qquad+\lim_{n\rightarrow\infty}\left(\frac{b(n+2)}{\lambda}-\frac{b(n+1)}{\lambda}\right)\\
&=\frac{1}{2}-\frac{1}{2} + \lim_{n\rightarrow\infty}\left(\frac{b(n+2)}{\lambda}-\frac{b(n+1)}{\lambda}\right)\\
&=\frac{1}{\lambda}\lim_{n\rightarrow\infty}\left[b(n+2)-b(n+1)\right]\qquad \square
\end{align*}
```


### Removing the infinite sum

For a preference function of the form:

$$
b(k) = \begin{cases}
g(k),&k<k_0\\
g(k_0) + \beta(k-k_0), &k\ge k_0
\end{cases}
$$ for $\beta>0, k_0\in\mathbb N$ we have that

```{=tex}
\begin{align*}
\hat\rho(\lambda) = \sum_{n=0}^\infty\prod_{i=0}^{n-1}\frac{b(i)}{\lambda+b(i)} &= \sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \sum_{n=k_0+1}^\infty\left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\prod_{i=k_0}^{n-1}\frac{g(k_0) + \beta(i-k_0)}{\lambda +g(k_0) + \beta(i-k_0)}\right)\\
&=\sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\sum_{n=k_0+1}^\infty\prod_{i=k_0}^{n-1}\frac{g(k_0) + \beta(i-k_0)}{\lambda +g(k_0) + \beta(i-k_0)}
\end{align*}
```
Now using the fact that:

$$
\prod_{i=0}^n(x+yi) = x^{n+1}\frac{\Gamma(\frac{x}{y}+n+1)}{\Gamma(\frac{x}{y})}
$$ and reindexing the product in the second sum

```{=tex}
\begin{align*}
\hat\rho(\lambda) &= \sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\sum_{n=k_0+1}^\infty\frac{\Gamma\left(\frac{g(k_0)}{\beta}+n-k_0\right)\Gamma\left(\frac{\lambda+g(k_0)}{\beta}\right)}{\Gamma\left(\frac{\lambda+g(k_0)}{\beta}+n-k_0\right)\Gamma\left(\frac{g(k_0)}{\beta}\right)}\\
&= \sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \frac{\Gamma\left(\frac{\lambda+g(k_0)}{\beta}\right)}{\Gamma\left(\frac{g(k_0)}{\beta}\right)}\left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\sum_{n=k_0+1}^\infty\frac{\Gamma\left(\frac{g(k_0)}{\beta}+n-k_0\right)}{\Gamma\left(\frac{\lambda+g(k_0)}{\beta}+n-k_0\right)}\\
&=\sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \frac{\Gamma\left(\frac{\lambda+g(k_0)}{\beta}\right)}{\Gamma\left(\frac{g(k_0)}{\beta}\right)}\left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\sum_{n=1}^\infty\frac{\Gamma\left(\frac{g(k_0)}{\beta}+n\right)}{\Gamma\left(\frac{\lambda+g(k_0)}{\beta}+n\right)}
\end{align*}
```
In order to simplify the infinite sum, consider:

```{=tex}
\begin{align*}
\sum_{n=0}^\infty\frac{\Gamma(n+x)}{\Gamma(n+x+y)} &=\frac{1}{\Gamma(y)}\sum_{n=0}^\infty \text{y}(n+x,y)\\
&=\frac{1}{\Gamma(y)}\sum_{n=0}^\infty\int_0^1t^{n+x-1}(1-t)^{y-1}at\\
&=\frac{1}{\Gamma(y)}\int_0^1 t^{x-1}(1-t)^{y-1}\sum_{n=0}^\infty t^n\,at\\
&=\frac{1}{\Gamma(y)}\int_0^1 t^{x-1}(1-t)^{y-1}\frac{1}{1-t}at\\
&=\frac{1}{\Gamma(y)}\int_0^1 t^{x-1}(1-t)^{y-2}at\\
&=\frac{1}{\Gamma(y)}\text{y}(x,y-1)\\
&= \frac{\Gamma(x)}{(y-1)\Gamma(x+y-1)}
\end{align*}
```
this infinite sum does not converge when $x\le1$ as each term is $O(n^{-x})$. We can now use this in $\hat\rho(\lambda)$:

```{=tex}
\begin{align*}
\hat\rho(\lambda) &= \sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \frac{\Gamma\left(\frac{\lambda+g(k_0)}{\beta}\right)}{\Gamma\left(\frac{g(k_0)}{\beta}\right)}\left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\left(\frac{\Gamma\left(\frac{g(k_0)}{\beta}\right)}{\left(\frac{\lambda}{\beta}-1\right)\Gamma\left(\frac{g(k_0)+\lambda}{\beta}-1\right)}-\frac{\Gamma\left(\frac{g(k_0)}{\beta}\right)}{\Gamma\left(\frac{g(k_0)+\lambda}{\beta}\right)}\right)\\
&=\sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\left(\frac{\Gamma\left(\frac{g(k_0)+\lambda}{\beta}\right)}{\left(\frac{\lambda}{\beta}-1\right)\Gamma\left(\frac{g(k_0)+\lambda}{\beta}-1\right)}-1\right)\\
&=\sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\left(\frac{\frac{g(k_0)+\lambda}{\beta}-1}{\frac{\lambda}{\beta}-1}-1\right)\\
&=\sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\left(\frac{g(k_0)+\lambda-\beta}{\lambda-\beta}-1\right)\\&=\sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \frac{g(k_0)}{\lambda-\beta}\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\qquad \square
\end{align*}
```



# References
<!-- the pattern below controls the placement of the references -->
:::{#refs}
:::
