% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  sn-basic,
]{sn-jnl}


\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}





%%%% Standard Packages

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%

%%%%

\raggedbottom
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[section]
\newtheorem{refsolution}{Solution}[section]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Learning Network Growth Mechanisms from Degree Distributions in Preferential Attachment Networks},
  pdfauthor={Thomas William Boughen; Clement Lee; Vianey Palacios Ramirez},
  pdfkeywords={networks, discrete extremes, power law, preferential
attachment},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title[Learning Network Growth Mechanisms from Degree Distributions in
Preferential Attachment Networks]{Learning Network Growth Mechanisms
from Degree Distributions in Preferential Attachment Networks}

% author setup
\author[1]{\fnm{Thomas William} \sur{Boughen}}\author[1]{\fnm{Clement} \sur{Lee}}\author[1]{\fnm{Vianey Palacios} \sur{Ramirez}}
% affil setup
\affil[1]{\orgdiv{School of Mathematics, Statistics and
Physics}, \orgname{Newcastle University}}

% abstract 

\abstract{Devising the underlying generating mechanism of a real-life
network is difficult as, more often than not, only its snapshots are
available, but not its full evolution. One candidate for the generating
mechanism is preferential attachment which, in its simplest form,
results in a degree distribution that follows the power law.
Consequently, the growth of real-life networks that roughly display such
power-law behaviour is commonly modelled by preferential attachment.
However, the validity of the power law has been challenged by the
presence of alternatives with comparable performance, as well as the
recent findings that the right tail of the degree distribution is often
lighter than implied by the body, whilst still being heavy. In this
paper, we study a modified version of the model with a flexible
preference function that allows super/sub-linear behaviour whilst also
guaranteeing that the limiting degree distribution has a heavy tail. We
relate the distributions tail index directly to the model parameters,
allowing direct inference of the parameters from the degree distribution
alone.}

% keywords
\keywords{networks,  discrete extremes,  power law,  preferential
attachment}

\begin{document}
\maketitle


\newpage

\section{Introduction}\label{introduction}

Networks have become very powerful tools for representing and analysing
complex systems, with uses in a large array of fields, from using
stochastic block models to detect communities online \citep{Latouche11},
to using Exponential Random Graph Models (ERGMs) to analyse the global
trade network \citep{Setayesh22}, and even using mechanistic models to
investigate the patterns in neural systems \citep{Betzel17}.

Along with the recent rise of interest in networks, has been a debate
amongst those that study them regarding the claims that most real
networks are scale-free. This claim is most often equivalent to claiming
that the degree distributions of real networks follow a power law, that
is the fraction of nodes with degree \(k\) is proportional to
\(k^{-\alpha}\), and therefore has a regularly varying tail with extreme
value index \(1/\alpha\). \citet{Broido_2019} provide empirical
evidence, using almost one thousand networks, that these scale-free
networks do not make up a large majority of real networks. They compare
the fits of a power law model against that of several non-scale-free
models only finding strong evidence for scale-freeness in four percent
and weak evidence in fifty-two percent of the networks. On the other
side of this debate is \citet{Voitalov_2019} who disagrees and claims
that these networks are not nearly as rare and only appear to so as a
result of an unrealistic expectation of a power law without deviations
or noise. Evidence of these deviations from a power law is shown by
\citet{Lee24} who demonstrate that a lot of networks are partially
scale-free, in that the body of the degree distribution is modelled well
by a power law while the tail is lighter than what is implied by the
body (although still regularly varying).

Most of the study into the degrees of real networks, including those
mentioned here, has been purely descriptive in the sense that no
information about the networks growth is revealed (aside from relating a
power law to preferential attachment).

The study of network growth began to gain popularity when
\citet{Barabasi99} showed that a network that has grown in such a way
that nodes gain edges at a rate proportional to their degree results in
a network that is scale-free. This is why the presence of real
scale-free networks is such a hot topic, since if it is shown that they
are often scale-free then a model such as preferential attachment gains
a lot of justification for being used to model network growth. The
preferential attachment model they introduced uses a preference
function, \(b(k)=k+1\) where \(k\) is a node in-degree, that is fairly
simple and provided the foundations for many modifications and
extensions to the model. \citet{krapivsky01} considered a more general
preference function of the form \(b(k) = (k+1)^\alpha\) and showed that
the degree distribution only has regular variation in the tail when
\(\alpha=1\), no regular variation when \(0<\alpha<1\) (and therefore
not power law), and when \(\alpha>1\) a finite number of nodes end up
with all edges after a certain point resulting in a degenerate degree
distribution. \citet{wang2022random} returns to a linear preference
function of the form \(b(k) = k+\varepsilon\) but adds the possibility
for reciprocal edges to be sent, this results in joint in-degree
out-degree distribution that is multivariate regular varying and has the
property of hidden regular variation. \citet{rudas07} follows in the
footsteps of \citet{krapivsky01} and considers a preferential attachment
tree with a preference function \(b(\cdot)\) and uses theory from
continuous time branching processes to derive a limiting degree
distribution in terms of the preference function \(b(\cdot)\). Research
in this area tends to only focus on the theoretical asymptotic results
of network growth models with little analysis of real networks. This
paper aims to address this gap, asking if a network is assumed to come
from a preferential attachment model can we use the degree distribution
alone to directly infer the model parameters?

It is important to think about how we intend to consider the tail of the
degree distributions, because if not done correctly we may end up
essentially discounting the effects of the largest degrees (often the
most influential nodes) deviating from a power law. For this reason we
will use methods from discrete extreme value theory, as they best
capture the nature of the data involved, more specifically we will use
results from \citet{shimura12} to study how the tail of the degree
distribution is affected by the preference function since we wish to
align what we do with the recent findings that the a lot of real degree
distributions are regularly varying.

The remainder of the paper is as follows: Section~\ref{sec-tail} gives a
detailed description of the preferential attachment model alongside the
theoretical results for the survival of the limiting degrees with a
focus on the tail behaviour in terms of the preference function
\(b(\cdot)\). It is shown that a regularly varying degree distribution
can only be the result of an asymptotically linear preference function.
This section also introduces a preference function that can guarantee
regular variation in the degrees while remaining flexible via a set of
parameters that can provide behaviour similar to that in
\citet{krapivsky01} up until a threshold. Section~\ref{sec-model}
utilises the preference function proposed at the end of
Section~\ref{sec-tail}, and illustrates how the extreme value index
(EVI) of the degree distribution varies with the model parameters.
Section~\ref{sec-sim} consists of a simulation study where networks are
simulated from the proposed model for various parameter combinations,
demonstrating that the parameters can be recovered from fitting the
model to only the degree distribution. Section~\ref{sec-real} fits the
model to some real data and provides posterior estimates for the
preference function. Section~\ref{sec-conc} concludes the article.

\newpage

\section{Tail Behaviour of Preferential Attachment
Model}\label{sec-tail}

The model that we will focus on in this paper is the General
Preferential Attachment model in \citet{rudas07} and is defined as
follows:

Starting at time \(t=0\) with an initial network of \(m\) vertices that
each have no edges, at times \(t=1,2,\ldots\) a new vertex is added to
the network bringing with it \(m\) directed edges (with the new vertex
as the source); the target for each of these edges are selected from the
vertices already in the network with weights proportional to some
preference function \(b(\cdot)\) of their degree, where
\(b: N \mapsto \mathbb R^+\setminus\{0\}\) is such that:

\begin{equation}\phantomsection\label{eq-condb2}{
\sum_{k=0}^\infty\frac{1}{b(k)} = \infty.
}\end{equation}

Special cases of this model include the Barabási-Albert (BA) model when
\(b(k) = k+\varepsilon\), which leads to a power-law degree distribution
with EVI \(\xi=1/2\), and the Uniform Attachment (UA) model where
\(b(k)=c\) leading to a degree distribution that is not regularly
varying.

The survival function of the limiting degree distribution, called the
limiting survival hereafter, under condition \ref{eq-condb2} can be
analytically derived in the case where \(m=1\).

Consider a continuous time branching process \(\zeta(t)\) driven by a
Markovian pure birth process, with \(\zeta(0)=0\) abd birth rates
depending on a non-negative function \(b(\cdot)\):

\[
\Pr(\zeta(t+dt)=k+1|\zeta(t)=k) = b(k)dt + o(dt).
\] Now let \(\Upsilon(t)\) be the tree determined by \(\zeta(t)\) as
follows: \(\Upsilon(t)=\{\emptyset\}\) and \(\Upsilon(t)=G\) where each
existing node \(x\) in \(\Upsilon(t)\) gives birth to a child with rate
\(b(\mathrm{deg}(x, \Upsilon(t))\) independently of the other nodes
where \(\mathrm{deg}(x, \Upsilon(t))\) is the degree of node \(x\) in
the tree \(\Upsilon(t)\) at time \(t\).

Theorem 1 from \citet{rudas07} states that for the tree \(\Upsilon(t)\)
at time \(t\):

\begin{equation}\phantomsection\label{eq-survlim}{
\lim_{t\rightarrow\infty}\frac{1}{|\Upsilon(t)|}\sum_{x\in\Upsilon(t)}\varphi(\Upsilon(t)_{\downarrow x}) = \lambda^* \int_0^\infty e^{-\lambda^* t}\mathbb E\left[\varphi(\Upsilon(t))\right]dt
}\end{equation} where \(\lambda^*\) satisfies \(\hat\rho(\lambda^*)=1\)
and \(\hat\rho\) is the Laplace transform of the density of the point
process associated with the pure birth process that corresponds to a
nodes individual growth, that is:

\[
\hat\rho(\lambda) \coloneq \int_0^\infty e^{-\lambda t}\rho(t)\mathrm{d}t.
\]

The limiting survival can be viewed as the limit of the empirical
proportion of vertices with degree over a threshold \(k\in\mathbb N\),
that is:

\[
\bar F(k) = \lim_{t\rightarrow\infty}\frac{\sum_{x\in\Upsilon(t)}\mathbb I\left\{\text{deg}(x,\Upsilon(t)_{\downarrow x})>k\right\}}{\sum_{x\in\Upsilon(t)} 1}
\] which can also be written using Equation~\ref{eq-survlim} as:

\[
\bar F(k) = \frac{\int_0^\infty e^{-\lambda^* t}\mathbb E\left[\mathbb I\left\{\text{deg}(x,\Upsilon(t))>k\right\}\right]dt}{\int_0^\infty e^{-\lambda^* t}dt} = \prod_{i=0}^k\frac{b(i)}{\lambda^* + b(i)}.
\] Therefore, the corresponding probability mass function of the degree
distribution \(f(k) = \bar F(k-1) - \bar F(k)\) is

\[
f(k) = \frac{\lambda^*}{\lambda^* + b(k)}\prod_{i=0}^{k-1}\frac{b(i)}{\lambda^*+b(i)}.
\]

We are interested in how the tail behaviour of the discrete limiting
degree distribution is affected by the preference function \(b\).

For a distribution \(F\) with survival function \(\bar F\) and some
\(k\in\mathbb Z^+\) let

\[
\Omega(F,k) = \left(\log\displaystyle\frac{\bar F (k+1)}{\bar F (k+2)}\right)^{-1} - \left(\log\displaystyle\frac{\bar F (k)}{\bar F (k+1)}\right)^{-1}.
\]

\citet{shimura12} states that if
\(\lim_{k\rightarrow\infty} \Omega(F,k) = 1/\alpha\) (\(\alpha>0\)),
then \(F\) is regularly varying with \(\bar F(k) \sim k^{-\alpha}\). On
the other hand, if \(\lim_{k\rightarrow\infty} \Omega(F,k) = 0\) then we
will refer to the distribution as light-tailed. This allows us to show
the following:

\begin{proposition}[]\protect\hypertarget{prp-omega}{}\label{prp-omega}

If \(\bar F(k) = \prod_{i=0}^k\frac{b(i)}{\lambda^* + b(i)}\) and
\(b(k) \rightarrow \infty\) as \(k\rightarrow \infty\), then \[
\lim_{k\rightarrow\infty}\Omega(F,k) = \lim_{k\rightarrow\infty}\frac{b(k+1)-b(k)}{\lambda^*}.
\]

\end{proposition}

See Appendix \ref{sec-proofs} for the details of the proof.

Proposition~\ref{prp-omega} aligns with the result from
\citet{krapivsky01} demonstrating that a sub-linear preference function
will lead to a light-tailed distribution, as
\(\lim_{k\rightarrow\infty} b(k+1)-b(k) = 0\) if \(b(k)=k^\alpha\) where
\(\alpha < 1\). Proposition 2.1 also aligns with the fact that BA model
produces a regularly varying degree distribution with EVI \(\xi=0.5\) by
considering the preference function \(b(k) = k + \varepsilon\), as
\(\lim_{k\rightarrow\infty}b(k+1)-b(k)=1\) leaving the tail index to be
\(1/\lambda^*\) which using \(\hat\rho\) can be found to be \(1/2\). So,
in order for the degree distribution to be regularly varying we need
that the limit \(\lim_{k\rightarrow\infty} b(k+1)-b(k)\) exists and is
positive. To determine the class of functions that will result in
regularly varying limiting degree distributions, we use the following
result:

\begin{proposition}[]\protect\hypertarget{prp-omega2}{}\label{prp-omega2}

Consider a GPA model with preference function \(b(\cdot)\) satisfying
Proposition~\ref{prp-omega}. Then the limiting degree distribution is
regularly varying with EVI \(c/\lambda^{*}\) if and only if
\(\lim_{k\rightarrow\infty}b(k)/k = c > 0\).

\emph{Proof}

From Proposition~\ref{prp-omega}, we have that: \[
\lim_{k\rightarrow\infty}[b(k+1)-b(k)] = c>0.
\] Now, setting \(b_k = b(k)\) and \(a_k = k\):

\[
\lim_{k\rightarrow\infty}[b(k+1)-b(k)] = \lim_{k\rightarrow\infty}\frac{b_{k+1} - b_k}{a_{k+1} - a_k} = c>0,
\] meaning by Theorem~\ref{thm-stolz} in Appendix \ref{sec-sup}:

\[
\lim_{k\rightarrow\infty}\frac{b_k}{a_k} = \lim_{k\rightarrow\infty}\frac{b(k)}{k} = c>0\qquad \square.
\]

\end{proposition}

Using this result we can understand how the preference function is
directly connected with the EVI and regular variation of the degree
distribution. We use this result to create a preference function that
guarantees regular variation in the tail of the degree distribution,
aligning with analysis of real networks, whilst allowing for the tail to
deviate from the shape implied by the body. This gives the model the
capability to produce realistic behaviour in the degrees like what was
in \citet{Lee24} by using a piecewise function inspired by the observed
deviation from the power law after a certain threshold.

\begin{equation}\phantomsection\label{eq-pref}{
b(k) = \begin{cases}
k^\alpha + \varepsilon,&k<k_0,\\
k_0^\alpha + \varepsilon + \beta(k-k_0), &k\ge k_0
\end{cases}
}\end{equation} for \(\alpha,\beta, \varepsilon>0\) and
\(k_0\in\mathbb N\).

This preference function, as per Proposition~\ref{prp-omega}, will
produce a degree distribution with EVI \(\xi=\beta/\lambda^*\)
guaranteeing regular variation. We study this preference function
further in the next section.

\newpage

\section{A Preferential Attachment Model with Flexible Regular
Variation}\label{sec-model}

In the previous section, we found that using a preference function with
linearity in the limit allows for the inclusion of sub/super-linear
behaviour below the threshold, while simultaneously guaranteeing regular
varation of the degrees. In this section, we study how the limiting
degree distribution varies with the preference function according to
Equation~\ref{eq-pref}. Its survival is

\begin{equation}\phantomsection\label{eq-polysurv}{
\bar F(k) = \begin{cases}
\prod_{i=0}^{k}\frac{i^\alpha + \varepsilon}{\lambda^*+i^\alpha + \varepsilon},&k<k_0,\\
\left(\prod_{i=0}^{k_0-1}\frac{i^\alpha + \varepsilon}{\lambda^*+i^\alpha + \varepsilon}\right)\frac{\Gamma(\lambda^*+k_0^\alpha + \varepsilon)/\beta)}{\Gamma\left((k_0^\alpha + \varepsilon)/\beta\right)} \frac{\Gamma\left(k-k_0 + 1 +\frac{k_0^\alpha + \varepsilon}{\beta}\right)}{\Gamma\left(k-k_0 + 1 +\frac{\lambda^* +k_0^\alpha + \varepsilon}{\beta}\right)},&k\ge k_0,
\end{cases}
}\end{equation}

with \(\lambda^*\) satisfying \(\hat \rho(\lambda^*)=1\) where

\begin{equation}\phantomsection\label{eq-rho}{
\hat\rho(\lambda) = \sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{i^\alpha + \varepsilon}{\lambda+i^\alpha + \varepsilon} + \left(\frac{k_0^\alpha + \varepsilon}{\lambda-\beta}\right)\prod_{i=0}^{k_0-1}\frac{i^\alpha + \varepsilon}{\lambda + i^\alpha + \varepsilon} 
}\end{equation}

which must be solved numerically for most parameter choices. Also, note
that \(\lambda>\beta\).

Some examples of the limiting degree distribution for various parameter
combinations are shown below on log-log scale in
Figure~\ref{fig-polylinsurv}:

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{paper_files/figure-pdf/fig-polylinsurv-1.pdf}

}

\caption{\label{fig-polylinsurv}Theoretical survival distributions of
the limiting degree distributions, according to various combinations of
\((\alpha, \beta, \varepsilon)\) and \(k_0=20\) of the proposed
preferential attachment model.}

\end{figure}%

Figure~\ref{fig-polylinsurv} demonstrates that this model can capture a
wide variety of shapes for the survival function, including a large
range of possible tail indices ranging from 0.035
(\(\alpha=1.5, \beta=0.1, \varepsilon=1\)) to 0.999
(\(\alpha=0.5, \beta=1.5, \varepsilon=0.01\)).

The survival function (\ref{eq-polysurv}) can be connected to the
discrete version of the generalised Pareto distribution (GP), called the
Integer GP (IGP) \citep{Rohrbeck_2018} with conditional survival: \[
\Pr(X> x|X> v) = \left(\frac{\xi(x-v)}{\sigma} + 1\right)^{-1/\xi},\qquad x=v+1,v+2,\ldots
\] for \(v\in\mathbb Z^+, \sigma>0,\xi\in \mathbb R\), denoted as
\(X|X>u \sim  \mathrm {IGP}(\xi, \sigma, u)\) where \(\xi\) is the shape
parameter controlling the tail index

By Equation~\ref{eq-polysurv} and using Stirling's approximation:

\begin{align*}
\bar F(k|k\ge k_0) &= \frac{\Gamma\left(\frac{\lambda^* + k_0^\alpha + \varepsilon}{\beta}\right)}{\Gamma\left(\frac{k_0^\alpha + \varepsilon}{\beta}\right)}\times\frac{\Gamma\left(k-k_0  +1 + \frac{k_0^\alpha + \varepsilon}{\beta}\right)}{\Gamma\left(k-k_0  +1 + \frac{\lambda^*+ k_0^\alpha + \varepsilon}{\beta}\right)}\\
&\approx\left(\frac{k_0^\alpha+\varepsilon}{\beta}\right)^{\lambda^*/\beta}\left(k-k_0+1+\frac{k_0^\alpha + \varepsilon}{\beta}\right)^{-\lambda^*/\beta}\\
&=\left(\frac{k_0^\alpha+\varepsilon}{k_0^\alpha+\varepsilon + \beta}\right)^{\lambda^*/\beta}\left(\frac{\beta(k-k_0)}{\beta + k_0^\alpha+\varepsilon} + 1\right)^{-\lambda^*/\beta}\\
&=\left(\frac{\beta(k+1-k_0)}{k_0^{\alpha}+\varepsilon} + 1\right)^{-\lambda^{*}/\beta}
\end{align*}

Therefore, \[
\bar F(k) 
\begin{cases}
=\prod_{i=0}^{k}\frac{i^\alpha + \varepsilon}{\lambda^*+i^\alpha + \varepsilon},&k<k_0,\\
\approx \left(\prod_{i=0}^{k_0-1}\frac{i^\alpha + \varepsilon}{\lambda^*+i^\alpha + \varepsilon}\right) \left(\frac{\beta(k+1-k_0)}{k_0^{\alpha}+\varepsilon} + 1\right)^{-\lambda^*/\beta},&k\ge k_0,
\end{cases}
\] meaning that for \(k\ge k_0\) the limiting degree distribution (for
large \(k_0^\alpha\)) is approximated by
\(\text{IGP}\left(\frac{\beta}{\lambda^*}, \frac{k_0^\alpha + \varepsilon}{\lambda^*},k_0-1\right)\).

To assess how close of an approximation this is, the theoretical
conditional survivals are shown in Figure~\ref{fig-approx_surv} in
colour and their IGP approximations are shown in grey. The approximation
seems to hold up fairly well even for large degrees and more so when
\(\alpha\) is larger.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{paper_files/figure-pdf/fig-approx_surv-1.pdf}

}

\caption{\label{fig-approx_surv}Theoretical conditional survivals (grey)
alongside their IGP approximations (coloured).}

\end{figure}%

In agreement with Proposition~\ref{prp-omega2}, \(\beta>0\) ensures that
the shape parameter of the IGP is positive and thus the distribution is
regularly varying. Additionally the value of the shape parameter \(\xi\)
is shown in Figure~\ref{fig-polyheat} for various parameter choices. The
darker and lighter regions on the heat maps correspond to a heavier and
a lighter tail, respectively, the red dashed line shows combinations of
\(\alpha\) and \(\beta\) that produce a limiting degree distribution
with the same tail index as the Barabási-Albert model, that is,
\(\xi=0.5\).

Through the connection to the IGP, fitting this model is almost
equivalent to fitting the IGP to the degree and estimating the
parameters but instead of only describing the shape of the degree
distribution we would also gain estimates for the shape of the
preference function while can help understand the mechanisms underlying
the growth of the network.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{paper_files/figure-pdf/fig-polyheat-1.pdf}

}

\caption{\label{fig-polyheat}Heat maps of \(\xi\) for various
combinations of the parameters of the proposed model.}

\end{figure}%

\newpage

To perform inference of the model parameters, we consider a network with
degree count vector \(\pmb n = (n_0, n_1, \ldots, n_M)\), where \(M\) is
the maximum degree. We can then write the likelihood as:

\begin{align*}
L(\pmb x,\pmb n | \pmb \theta) = &\left(\frac{\lambda^*}{\lambda^*+\varepsilon}\right)^{n_0}\left(\prod_{j=l}^{k_0-1}\frac{j^\alpha +\varepsilon}{\lambda^* + j^\alpha +\varepsilon}\right)^{\left(\sum_{i\ge k_0}n_{i}\right)} \\ &\times \prod_{l \le i<k_0}\left(\frac{\lambda^*}{\lambda^* +i^\alpha + \varepsilon } \prod_{j=l}^{k_0-1}\frac{j^\alpha + \varepsilon}{\lambda^* + j^\alpha + \varepsilon}\right)^{n_i}\\ &\times \prod_{i\ge k_0}\left(\frac{\text{B}(i-k_0 + (k_0^\alpha + \varepsilon)/\beta,1+\lambda^*/\beta)}{\text{B}((k_0^\alpha + \varepsilon)/\beta,\lambda^*/\beta)}\right)^{n_i}
\end{align*}\label{eq-lh}

where \(B(y,z)\) is the the beta function, and \(l\ge0\) is a variable
that allows truncating the data such that the minimum degree is \(l\).
This will allow the model to be fitted whilst ignoring the influence of
the lower degrees (those less than \(l\)) as the model does not capture
the behaviour well at the lower degrees since \citet{rudas07} only
provides results for the case of a preferential attachment tree.

\section{Applications}\label{applications}

\subsection{Simulated Data}\label{sec-sim}

This first subsection aims to show that the parameters of the model (and
therefore the preference function) in Section~\ref{sec-model} can be
recovered from simulating a network from the model, and fitting it to
the observed degree distribution, using the likelihood in \eqref{eq-lh}.

The procedure for recovering the parameters begins with simulating a
network from the model with \(N=100,000\) vertices and \(m=1\) given
some set of parameters
\(\pmb\theta = (\alpha, \beta, \varepsilon, k_0)\), obtaining the degree
counts and using the likelihood from the previous section alongisde the
priors:

\begin{align*}
\alpha&\sim \text{Gamma}(1,0.01),\\
\beta &\sim  \text{Gamma}(1,0.01),\\
k_0 &\sim \text{U}(1,10,000),\\
\varepsilon &\sim \text{Gamma(1,0.01)},
\end{align*}

where Gamma(a, b) is the Gamma distribution with shape a and rate b, and
U(a, b) is uniform distribution with lower and upper bounds a and b, to
obtain a posterior distribution, up to the proportionality constant.
Posterior samples can then be obtained by an adaptive
Metropolis-Hastings Markov chain Monte Carlo (MCMC) algorithm. For these
simulated networks \(l=0\).

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{paper_files/figure-pdf/fig-rec1-1.pdf}

}

\caption{\label{fig-rec1}Posterior estimates of survival function for
data simulated from the proposed model with various combinations of
(\(\alpha\),\(\beta\),\(\varepsilon\)) and \(k_0=20\).}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/mcmc_plot.jpg}

}

\caption{\label{fig-rec2}: Posterior estimates of paramters for data
simulated from the proposed model with various combinations of
(\(\alpha\),\(\beta\),\(\varepsilon\)) and \(k_0=20\).}

\end{figure}%

Figure~\ref{fig-rec1} and Figure~\ref{fig-rec2} shows the estimations
demonstrates the usefulness of the model, as we can recover the model
parameters well from only the final degree distribution of the simulated
network. This indicates that the method may also be applied to the
degree distributions of real networks, estimating the model parameters
assuming they evolved according to the GPA scheme.

\subsection{Real Data}\label{sec-real}

In this section, we fit the proposed model to the degree distributions
of various real networks and learn about the mechanics of their growth.
While we also compare the fit to that of the mixture distribution by
\citet{Lee24} we note that the proposed method has the additional
benefit of learning directly about the growth of a network from the
inference results. The data consists of 12 networks sourced from
\href{konect.cc}{KONECT} and the
\href{https://networkrepository.com}{Network Data Repository}\citep{nr}:

\begin{itemize}
\tightlist
\item
  \texttt{as-caida20071105}: network of autonomous systems of the
  Internet connected with each other from the CAIDA project
\item
  \texttt{dimacs10-astro-ph} : co-authorship network from the
  ``astrophysics'' section (astro-ph) of arXiv
\item
  \texttt{ego-twitter}: network of twitter followers
\item
  \texttt{facebook-wosn-wall}: subset of network of Facebook wall posts
\item
  \texttt{maayan-faa}: USA FAA (Federal Aviation Administration)
  preferred routes as recommended by the NFDC (National Flight Data
  Center)
\item
  \texttt{maayan-Stelzl}: network representing interacting pairs of
  proteins in humans
\item
  \texttt{moreno-blogs-blogs}: network of URLs found on the first pages
  of individual blogs
\item
  \texttt{opsahl−openflights}: network containing flights between
  airports of the world.
\item
  \texttt{pajek-erdos}: co-authorship network around Paul Erd\o``s
\item
  \texttt{reactome}: network of protein--protein interactions in humans
\item
  \texttt{sx-mathoverflow}: interactions from the StackExchange site
  \href{https://mathoverflow.net/}{MathOverflow}
\item
  \texttt{topology}: network of connections between autonomous systems
  of the Internet
\end{itemize}

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{paper_files/figure-pdf/fig-real1-1.pdf}

}

\caption{\label{fig-real1}Posterior estimates (solid red) of survival
for several real data sets and their 95\% credible intervals (dotted
red).}

\end{figure}%

Figure~\ref{fig-real1} displays the posterior estimates of the survival
function for various data sets, obtained from fitting the GPA model and
the Zipf-IGP mixture model from \citet{Lee24}. In most cases, the GPA
model gives a similar fit to the Zipf-IGP model but where the GPA model
fits well we gain additional information about the preference function
assuming that the network evolved according the the GPA scheme.

Figure~\ref{fig-shapes} shows the posterior of the shape parameter
\(\xi\) obtained from the Zipf-IGP model alongside the posterior of the
equivalent shape parameter \(\beta/\lambda^*\) obtained from fitting the
GPA model. Generally, the GPA model performs similarly to the Zipf-IGP
when estimating the tail behaviour of the degree distribution. In the
cases of substantial discrepancies, it is either because the GPA model
fits the tail better than the Zipf-IGP model does, or because of the
threshold being estimated as too low forcing almost all of the data to
be modelled by the linear part of the GPA. This again shows the effects
that small degrees have on this model, which is somewhat expected as the
theory used for this model is for trees and none of these real networks
(nor many real networks) are.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{paper_files/figure-pdf/fig-shapes-1.pdf}

}

\caption{\label{fig-shapes}Posterior estimates (solid red) of survival
for several real data sets and their 95\% credible intervals (dotted
red).}

\end{figure}%

Figure~\ref{fig-pa} shows the estimated preference function \(b(k)\)
alongside the 95\% credible interval on a log-log plot. Although the
credible interval becomes very large for the largest degrees, this is
expected as not all of these networks had data in that region, and for
those that do the credible interval is much narrower, as is the case for
\texttt{sx-mathoverflow}. Looking at the shape of the preference
function, there appears to be two distinct shapes of preference
function. The first appears mostly flat (similar to uniform attachment)
for the smallest degrees and then after a threshold preferential
attachment kicks in, some with this shape are \texttt{pajek-erdos} and
\texttt{sx-mathoverflow}. The second distinct shape appears to provide
some clear preferential attachment behaviour that then slows down after
a certain point, examples of this are seen in the two infrastructure
networks \texttt{opsahl-openflights} and \texttt{topology}. This slowing
down could be viewed as a kind of diminishing returns on the degree of a
vertex i.e.~as a vertex gets larger gaining more connections has less of
an effect than it did before some threshold \(k_0\).

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{paper_files/figure-pdf/fig-pa-1.pdf}

}

\caption{\label{fig-pa}Posterior estimate for preference function
(solid) with 95\% credible interval (dashed) on log-log scale.}

\end{figure}%

\newpage

\section{Conclusion and Discussion}\label{sec-conc}

In this paper we introduced a class of preference functions that, under
the GPA scheme, generate a network with a flexible yet heavy-tailed
degree distribution. From the simulation study we showed that the
parameters can be recovered from fitting the model to the degrees alone.
We also applied this method to the degree distributions of real
networks, estimating their model parameters assuming they evolved in the
same way. Not only did this yield fairly good fits for the degree
distribution, similar to that of the Zipf-IGP, it also came with the
added benefit of giving a posterior estimate for a preference function.

One limitation of this method is that the lowest degrees needed to be
truncated as they had a very large effect on the fit of the model as a
result of using theory developed for trees and applying it to general
networks. Future work could apply theory developed for general networks
using a similar method to this, allowing us to compare the results here
something that is more accurate. This could include fixing the
out-degree of new nodes at a constant greater than one, or allowing the
out-degree of new nodes to vary.

\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

\newpage

\section{Supplementary Results}\label{sec-sup}

\begin{theorem}[Stolz-Cesàro Theorem
\citep{cesaro}]\protect\hypertarget{thm-stolz}{}\label{thm-stolz}

Let \((a_k)_{k\ge1}\) and \((b_k)_{k\ge1}\) be two sequence of real
numbers. Assume \((a_k)_{k\ge1}\) is a strictly monotone and divergent
sequence and the following limit exists:

\[
\lim_{k\rightarrow\infty}\frac{b_{k+1} - b_k}{a_{k+1} - a_k} = l.
\] Then,

\[
\lim_{k\rightarrow\infty}\frac{b_k}{a_k} = l.
\]

\end{theorem}

\section{Additional Plots}\label{additional-plots}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/pars_plot.png}

}

\caption{: Posterior estimates of parameters for real data.}

\end{figure}%

\section{Proofs and Derivations}\label{sec-proofs}

\subsection{\texorpdfstring{Proof of
Proposition~\ref{prp-omega}}{Proof of Proposition~}}\label{proof-of-prp-omega}

Taking the form of the GPA degree survival: \[
\bar F(k) = \prod_{i=0}^k\frac{b(i)}{\lambda+b(i)}
\] and substituting into the formula for \(\Omega(F,n)\):

\begin{align*}
\Omega(F,k)&=\left(\log\frac{\prod_{i=0}^{k+1}\frac{b(i)}{\lambda+b(i)}}{\prod_{i=0}^{k+2}\frac{b(i)}{\lambda+b(i)}}\right)^{-1}-\left(\log\frac{\prod_{i=0}^{k}\frac{b(i)}{\lambda+b(i)}}{\prod_{i=0}^{k+1}\frac{b(i)}{\lambda+b(i)}}\right)^{-1}\\
&=\left(\log\frac{\lambda+b(k+2)}{b(k+2)}\right)^{-1}-\left(\log\frac{\lambda+b(k+1)}{b(k+1)}\right)^{-1}\\
&=\left(\log\left[1+\frac{\lambda}{b(k+2)}\right]\right)^{-1}-\left(\log\left[1+\frac{\lambda}{b(k+1)}\right]\right)^{-1}.
\end{align*}

Clearly if \(b(k)=c\) or \(\lim_{k\rightarrow\infty}b(k)=c\) for some
\(c>0\) then \(\Omega(F,k)=0\). Now consider a non-constant \(b(k)\) and
re-write \(\Omega(F,k)\) as:

\begin{align*}
\Omega(F,k) &= \left(\log\left[1+\frac{\lambda}{b(k+2)}\right]\right)^{-1}-\frac{b(k+2)}{\lambda}+\frac{b(k+2)}{\lambda}-\left(\log\left[1+\frac{\lambda}{b(k+1)}\right]\right)^{-1}\\&+\frac{b(k+1)}{\lambda}  -\frac{b(k+1)}{\lambda}\\
&=\left\{ \left(\log\left[1+\frac{\lambda}{b(k+2)}\right]\right)^{-1}-\frac{b(k+2)}{\lambda}\right\} - \left\{ \left(\log\left[1+\frac{\lambda}{b(k+1)}\right]\right)^{-1}-\frac{b(k+1)}{\lambda}\right\}\\&+\frac{b(k+2)}{\lambda}-\frac{b(k+1)}{\lambda}.
\end{align*}

Then if \(\lim_{k\rightarrow\infty}b(k)=\infty\) it follows that:

\begin{align*}
\lim_{k\rightarrow\infty}\Omega(F,k) &= \lim_{k\rightarrow\infty}\left\{ \left(\log\left[1+\frac{\lambda}{b(k+2)}\right]\right)^{-1}-\frac{b(k+2)}{\lambda}\right\} \\&- \lim_{k\rightarrow\infty}\left\{ \left(\log\left[1+\frac{\lambda}{b(k+1)}\right]\right)^{-1}-\frac{b(k+1)}{\lambda}\right\}\\
&\qquad+\lim_{k\rightarrow\infty}\left(\frac{b(k+2)}{\lambda}-\frac{b(k+1)}{\lambda}\right)\\
&=\frac{1}{2}-\frac{1}{2} + \lim_{k\rightarrow\infty}\left(\frac{b(k+2)}{\lambda}-\frac{b(k+1)}{\lambda}\right)\\
&=\frac{1}{\lambda}\lim_{k\rightarrow\infty}\left[b(k+2)-b(k+1)\right].\qquad \square
\end{align*}

\subsection{\texorpdfstring{Derivation of
Equation~\ref{eq-rho}}{Derivation of Equation~}}\label{derivation-of-eq-rho}

For a preference function of the form:

\[
b(k) = \begin{cases}
g(k),&k<k_0,\\
g(k_0) + \beta(k-k_0), &k\ge k_0,
\end{cases}
\] for \(\beta>0, k_0\in\mathbb N\) we have that

\begin{align*}
\hat\rho(\lambda) &= \sum_{n=0}^\infty\prod_{i=0}^{n-1}\frac{b(i)}{\lambda+b(i)}\\ &= \sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \sum_{n=k_0+1}^\infty\left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\prod_{i=k_0}^{n-1}\frac{g(k_0) + \beta(i-k_0)}{\lambda +g(k_0) + \beta(i-k_0)}\right)\\
&=\sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\sum_{n=k_0+1}^\infty\prod_{i=k_0}^{n-1}\frac{g(k_0) + \beta(i-k_0)}{\lambda +g(k_0) + \beta(i-k_0)}.
\end{align*}

Now using the fact that:

\[
\prod_{i=0}^n(x+yi) = x^{n+1}\frac{\Gamma(\frac{x}{y}+n+1)}{\Gamma(\frac{x}{y})}
\] and reindexing the product in the second sum,

\begin{align*}
\hat\rho(\lambda) &= \sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\sum_{n=k_0+1}^\infty\frac{\Gamma\left(\frac{g(k_0)}{\beta}+n-k_0\right)\Gamma\left(\frac{\lambda+g(k_0)}{\beta}\right)}{\Gamma\left(\frac{\lambda+g(k_0)}{\beta}+n-k_0\right)\Gamma\left(\frac{g(k_0)}{\beta}\right)}\\
&= \sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \frac{\Gamma\left(\frac{\lambda+g(k_0)}{\beta}\right)}{\Gamma\left(\frac{g(k_0)}{\beta}\right)}\left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\sum_{n=k_0+1}^\infty\frac{\Gamma\left(\frac{g(k_0)}{\beta}+n-k_0\right)}{\Gamma\left(\frac{\lambda+g(k_0)}{\beta}+n-k_0\right)}\\
&=\sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \frac{\Gamma\left(\frac{\lambda+g(k_0)}{\beta}\right)}{\Gamma\left(\frac{g(k_0)}{\beta}\right)}\left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\sum_{n=1}^\infty\frac{\Gamma\left(\frac{g(k_0)}{\beta}+n\right)}{\Gamma\left(\frac{\lambda+g(k_0)}{\beta}+n\right)}.
\end{align*}

In order to simplify the infinite sum, consider:

\begin{align*}
\sum_{n=0}^\infty\frac{\Gamma(n+x)}{\Gamma(n+x+y)} &=\frac{1}{\Gamma(y)}\sum_{n=0}^\infty \text{B}(n+x,y)\\
&=\frac{1}{\Gamma(y)}\sum_{n=0}^\infty\int_0^1t^{n+x-1}(1-t)^{y-1}at\\
&=\frac{1}{\Gamma(y)}\int_0^1 t^{x-1}(1-t)^{y-1}\sum_{n=0}^\infty t^n\,at\\
&=\frac{1}{\Gamma(y)}\int_0^1 t^{x-1}(1-t)^{y-1}\frac{1}{1-t}at\\
&=\frac{1}{\Gamma(y)}\int_0^1 t^{x-1}(1-t)^{y-2}at\\
&=\frac{1}{\Gamma(y)}\text{y}(x,y-1)\\
&= \frac{\Gamma(x)}{(y-1)\Gamma(x+y-1)}.
\end{align*}

This infinite sum does not converge when \(x\le1\) as each term is
\(O(n^{-x})\). We can now use this in \(\hat\rho(\lambda)\):

\begin{align*}
\hat\rho(\lambda) &= \sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \frac{\Gamma\left(\frac{\lambda+g(k_0)}{\beta}\right)}{\Gamma\left(\frac{g(k_0)}{\beta}\right)}\left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\left(\frac{\Gamma\left(\frac{g(k_0)}{\beta}\right)}{\left(\frac{\lambda}{\beta}-1\right)\Gamma\left(\frac{g(k_0)+\lambda}{\beta}-1\right)}-\frac{\Gamma\left(\frac{g(k_0)}{\beta}\right)}{\Gamma\left(\frac{g(k_0)+\lambda}{\beta}\right)}\right)\\
&=\sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\left(\frac{\Gamma\left(\frac{g(k_0)+\lambda}{\beta}\right)}{\left(\frac{\lambda}{\beta}-1\right)\Gamma\left(\frac{g(k_0)+\lambda}{\beta}-1\right)}-1\right)\\
&=\sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\left(\frac{\frac{g(k_0)+\lambda}{\beta}-1}{\frac{\lambda}{\beta}-1}-1\right)\\
&=\sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \left(\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}\right)\left(\frac{g(k_0)+\lambda-\beta}{\lambda-\beta}-1\right)\\&=\sum_{n=0}^{k_0}\prod_{i=0}^{n-1}\frac{g(i)}{\lambda+g(i)} + \frac{g(k_0)}{\lambda-\beta}\prod_{i=0}^{k_0-1}\frac{g(i)}{\lambda+g(i)}.\qquad \square
\end{align*}

\newpage

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\renewcommand{\bibsection}{}
\bibliography{refs.bib}





\end{document}
